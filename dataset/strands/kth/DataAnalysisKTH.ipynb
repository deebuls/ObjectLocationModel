{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "description = pd.read_csv('cluster_new.txt',names=['number', 'desc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 37)\n"
     ]
    }
   ],
   "source": [
    "#File path of all the files\n",
    "test_filepath = \"./test/*.occ\"\n",
    "train_filepath = \"./train/*.occ\"\n",
    "\n",
    "#Creating a sample dataset adding both column time and data in it\n",
    "kth_dataset = pd.read_csv('./train/01.occ', sep=' ', names = ['time', 'data'])\n",
    "kth_test_dataset = pd.read_csv('./test/01.occ', sep=' ', names = ['time', 'data'])\n",
    "\n",
    "#Dropping data column since it will re read again\n",
    "kth_dataset = kth_dataset.drop('data', axis=1)\n",
    "kth_test_dataset = kth_test_dataset.drop('data', axis=1)\n",
    "\n",
    "#glob returns all the data, sorted sorts it, then read csv read each file \n",
    "for count, filename in enumerate(sorted(glob.glob(test_filepath))):\n",
    "    data = pd.read_csv(filename, sep = ' ', names = ['time', 'data'])\n",
    "    if any(description.number == count+1):\n",
    "        ddd = description['desc'][description['number'] == count+1 ].first_valid_index()\n",
    "        kth_test_dataset[ description.ix[ddd]['desc'] ] = data['data']\n",
    "    else:\n",
    "        kth_test_dataset[str(count+1)] = data['data']\n",
    "    \n",
    "kth_test_dataset['time'] = pd.to_datetime(kth_test_dataset['time'],unit='s')\n",
    "\n",
    "kth_test_dataset = kth_test_dataset.set_index('time')\n",
    "\n",
    "print (kth_test_dataset.shape)\n",
    "\n",
    "for count, filename in enumerate(sorted(glob.glob(train_filepath))):\n",
    "    data = pd.read_csv(filename, sep = ' ', names = ['time', 'data'])\n",
    "    if any(description.number == count+1):\n",
    "        ddd = description['desc'][description['number'] == count+1 ].first_valid_index()\n",
    "        kth_dataset[ description.ix[ddd]['desc'] ] = data['data']\n",
    "    else:\n",
    "        kth_dataset[str(count+1)] = data['data']\n",
    "    \n",
    "kth_dataset['time'] = pd.to_datetime(kth_dataset['time'],unit='s')\n",
    "\n",
    "kth_dataset = kth_dataset.set_index('time')\n",
    "kth_dataset['sep_hour']  = kth_dataset.index.hour\n",
    "kth_test_dataset['sep_hour'] = kth_test_dataset.index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nils chair</th>\n",
       "      <th>Johan chair</th>\n",
       "      <th>Yuquan chair</th>\n",
       "      <th>Yuquan laptop</th>\n",
       "      <th>Nils Backpack</th>\n",
       "      <th>trash can</th>\n",
       "      <th>Yuquan bag</th>\n",
       "      <th>Yuquan lamp</th>\n",
       "      <th>Yuquan water bottle</th>\n",
       "      <th>A bag on Johan's desk</th>\n",
       "      <th>...</th>\n",
       "      <th>calibration board</th>\n",
       "      <th>Nils bike helmet</th>\n",
       "      <th>Johan laptop</th>\n",
       "      <th>Nils cup</th>\n",
       "      <th>Johan jacket</th>\n",
       "      <th>box !!!</th>\n",
       "      <th>2nd backpack</th>\n",
       "      <th>couch</th>\n",
       "      <th>TV</th>\n",
       "      <th>sep_hour</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-08-20 13:17:46</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-20 15:04:53</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-28 15:19:34</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-28 16:16:04</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-08-28 17:16:25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Nils chair  Johan chair  Yuquan chair  Yuquan laptop  \\\n",
       "time                                                                        \n",
       "2014-08-20 13:17:46           1            1             1              1   \n",
       "2014-08-20 15:04:53           1            1             1              1   \n",
       "2014-08-28 15:19:34           1            1             1              0   \n",
       "2014-08-28 16:16:04           1            1             1              0   \n",
       "2014-08-28 17:16:25           1            1             1              0   \n",
       "\n",
       "                     Nils Backpack  trash can  Yuquan bag  Yuquan lamp  \\\n",
       "time                                                                     \n",
       "2014-08-20 13:17:46              0          0           1            1   \n",
       "2014-08-20 15:04:53              1          0           1            0   \n",
       "2014-08-28 15:19:34              1          0           0            0   \n",
       "2014-08-28 16:16:04              1          0           0            0   \n",
       "2014-08-28 17:16:25              1          0           0            0   \n",
       "\n",
       "                     Yuquan water bottle  A bag on Johan's desk     ...     \\\n",
       "time                                                                ...      \n",
       "2014-08-20 13:17:46                    1                       0    ...      \n",
       "2014-08-20 15:04:53                    0                       0    ...      \n",
       "2014-08-28 15:19:34                    0                       0    ...      \n",
       "2014-08-28 16:16:04                    0                       0    ...      \n",
       "2014-08-28 17:16:25                    0                       0    ...      \n",
       "\n",
       "                     calibration board  Nils bike helmet  Johan laptop  \\\n",
       "time                                                                     \n",
       "2014-08-20 13:17:46                  0                 0             0   \n",
       "2014-08-20 15:04:53                  0                 0             0   \n",
       "2014-08-28 15:19:34                  0                 0             0   \n",
       "2014-08-28 16:16:04                  0                 0             0   \n",
       "2014-08-28 17:16:25                  0                 0             0   \n",
       "\n",
       "                     Nils cup  Johan jacket  box !!!  2nd backpack  couch  TV  \\\n",
       "time                                                                            \n",
       "2014-08-20 13:17:46         0             0        0             0      0   0   \n",
       "2014-08-20 15:04:53         0             0        0             0      0   0   \n",
       "2014-08-28 15:19:34         0             0        0             0      0   0   \n",
       "2014-08-28 16:16:04         0             0        0             0      0   0   \n",
       "2014-08-28 17:16:25         0             0        0             0      0   0   \n",
       "\n",
       "                     sep_hour  \n",
       "time                           \n",
       "2014-08-20 13:17:46        13  \n",
       "2014-08-20 15:04:53        15  \n",
       "2014-08-28 15:19:34        15  \n",
       "2014-08-28 16:16:04        16  \n",
       "2014-08-28 17:16:25        17  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kth_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fbc069e9240>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEECAYAAADnD7WNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbc06ece6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.figure()\n",
    "kth_dataset.sep_hour.value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nils chair',\n",
       " 'Johan chair',\n",
       " 'Yuquan chair',\n",
       " 'Yuquan laptop',\n",
       " 'Nils Backpack',\n",
       " 'trash can',\n",
       " 'Yuquan bag',\n",
       " 'Yuquan lamp',\n",
       " 'Yuquan water bottle',\n",
       " \"A bag on Johan's desk \"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kth_dataset.drop('sep_hour', axis=1).sum().plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "zxc =  kth_dataset.drop('sep_hour', axis=1).sum() \n",
    "yyy = (zxc > 90 ) ^ (zxc < 10)  \n",
    "#yyy = (zxc < 10)  \n",
    "#list_to_learn = yyy[yyy == True].index.tolist()\n",
    "list_to_learn = yyy.index.tolist()\n",
    "list_to_learn[:10]\n",
    "#kth_dataset.drop('sep_hour', axis=1).sum() > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAFgCAYAAAB0avjBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf61YHUFV6PbzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f320134cb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kth_test_dataset.drop('sep_hour', axis=1).sum().plot(kind='bar')\n",
    "plt.show()\n",
    "kth_test_dataset.to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "kth_dataset[['11','12']]\n",
    "sns.heatmap(kth_dataset[['11', '12']].T)\n",
    "plt.xticks(rotation=-90) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'yaxis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-a13ffe2f4638>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkth_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'2014-09-21 16:32:03'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'2014-09-24 16:14:16'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrotation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_major_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmdates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDateFormatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%Y.%m.%d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'yaxis'"
     ]
    }
   ],
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "sns.heatmap(kth_dataset['2014-09-21 16:32:03':'2014-09-24 16:14:16'])\n",
    "plt.yticks(rotation=0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# build the figure\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(kth_dataset.T, ax=ax)\n",
    "\n",
    "# assign locator and formatter for the xaxis ticks.\n",
    "#ax.yaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "#ax.yaxis.set_major_formatter(mdates.DateFormatter('%Y.%m.%d'))\n",
    "\n",
    "# put the labels at 45deg since they tend to be too long\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=-90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beta-Bernouli Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ~ Bernoulli(p)\n",
      "  p = \n",
      "[ 0.2  0.9]\n",
      "\n",
      "(10,)\n",
      "Iteration 1: loglike=-9.397108e+00 (0.002 seconds)\n",
      " ~ Beta(a, b)\n",
      "  a = \n",
      "[  1.00100000e+00   1.00000000e-03]\n",
      "  b = \n",
      "[ 3.001  6.001]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAC1CAYAAAD86CzsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAwJJREFUeJzt2LGx2lAURVHJQwHSDO1RAtWoBPqT6OC5AuPEfwvDWulN\nTrSDO48xJgAav84eAPBNRBcgJLoAIdEFCIkuQOjy6ni73cayLNUWgP/e8/mcHo/H/Kf7y+guyzJd\nr9d/vwrgS3kvAIREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqIL\nEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJd\ngJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgNDl\n7AE/bZ7naV3Xs2e8jeM4pjHG2TPga318dNd1ne73+9kz3sa2bdO+72fPgK/lvQAQEl2AkOgChEQX\nICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6\nACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHR\nBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJ\nLkBIdAFCogsQEl2AkOgChEQXICS6ACHRBQiJLkBIdAFCogsQEl2A0OXsAT/tOI5p27azZ7yN4zjO\nngBf7eOjO8aY9n0/ewbANE3eCwAp0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQB\nQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqIL\nEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJdgJDoAoREFyAkugAh0QUIiS5ASHQBQqILEBJd\ngJDoAoQur47P57PaAfAR/tbNeYwRTQHAewEgJLoAIdEFCIkuQEh0AUKiCxD6DSrYLZTFc1S/AAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f543040a780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bayespy.nodes import Beta\n",
    "\n",
    "n_objects = 1\n",
    "n_timezones = 24\n",
    "n_observations = 1\n",
    "\n",
    "from bayespy.nodes import Bernoulli, Beta, Mixture, Categorical\n",
    "from bayespy.utils import random\n",
    "\n",
    "index = Categorical(np.ones(2)/2, plates=(10,)).random()\n",
    "p0 = np.array([0.6, 0.3])\n",
    "print (prob)\n",
    "observe_data = random.bernoulli(p0[index])\n",
    "p = Beta([1e-3, 1e-3], plates=(2,))\n",
    "z = Mixture(index, Bernoulli, p)\n",
    "#z = Bernoulli(p, plates=(10,2))\n",
    "print (z.plates)\n",
    "\n",
    "#observe_data = np.array([[0,1],[0,1],[0,1],[1,1],[1,1],\n",
    "#                [0,0], [0,0], [0,0], [0,0], [0,0]])\n",
    "z.observe(observe_data)\n",
    "\n",
    "from bayespy.inference import VB\n",
    "Q = VB(z, p)\n",
    "Q.update()\n",
    "\n",
    "print (p)\n",
    "import bayespy.plot as bpplt\n",
    "import numpy as np\n",
    "bpplt.hinton(p)\n",
    "#bpplt.pdf(p[0], np.linspace(0, 1, num=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start :  2016-07-28 09:34:12.586773\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Table cloth 2016-07-28 09:35:26.274307\n",
      "[ 0.10143308  0.10518872  0.10866295  0.10911524  0.11147932  0.11704032\n",
      "  0.12027148  0.10493025  0.09950387  0.10542149  0.10446879  0.1057911\n",
      "  0.10038186  0.09981756  0.10781447  0.10156152  0.10180245  0.10619195\n",
      "  0.11254859  0.11222403  0.10931945  0.11252654  0.10325944  0.10340256]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "27 2016-07-28 09:36:09.109986\n",
      "[ 0.09043629  0.09182991  0.0798827   0.08151375  0.08251725  0.0845087\n",
      "  0.08372675  0.08290433  0.078836    0.07770731  0.07691826  0.07910004\n",
      "  0.0857013   0.07433363  0.07813734  0.06832593  0.07432158  0.08036642\n",
      "  0.07954191  0.08711748  0.08879797  0.08334394  0.07865211  0.07541269]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Johan cap 2016-07-28 09:36:54.710641\n",
      "[ 0.14183179  0.12343155  0.12467469  0.11571034  0.1222928   0.12810153\n",
      "  0.11700729  0.12246452  0.12199159  0.13290375  0.12336528  0.12638247\n",
      "  0.13820469  0.11402569  0.12726862  0.11407454  0.11468757  0.11604726\n",
      "  0.12679173  0.12075509  0.13355224  0.13257924  0.12676969  0.12286445]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "calibration board 2016-07-28 09:37:29.938689\n",
      "[ 0.35729654  0.35124268  0.36825855  0.3513821   0.35191965  0.35597273\n",
      "  0.35511183  0.34658163  0.33211063  0.35731441  0.37517743  0.35358238\n",
      "  0.35011237  0.36322753  0.3460956   0.34116942  0.34946795  0.35048673\n",
      "  0.36865052  0.35274241  0.35498252  0.35420702  0.34933648  0.35712294]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Nils bike helmet 2016-07-28 09:38:21.804516\n",
      "[ 0.09263483  0.07387073  0.07199263  0.08383604  0.06960164  0.086879\n",
      "  0.07755507  0.08501875  0.07479871  0.07160293  0.08638078  0.09322829\n",
      "  0.0944685   0.0917262   0.06743249  0.07464119  0.07225235  0.08664267\n",
      "  0.07634807  0.07114928  0.09455247  0.08386859  0.07502815  0.08846919]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Johan laptop 2016-07-28 09:39:12.839022\n",
      "[ 0.29678539  0.30059648  0.30397216  0.3036278   0.30572157  0.30661208\n",
      "  0.30815749  0.30722602  0.2925065   0.30010878  0.30518467  0.33134678\n",
      "  0.2865351   0.30572998  0.285101    0.2796118   0.29247856  0.30183717\n",
      "  0.30344297  0.31933293  0.29703599  0.2939341   0.3015365   0.2909681 ]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Nils cup 2016-07-28 09:39:52.284512\n",
      "[ 0.05765101  0.0578543   0.05907935  0.05616216  0.05420846  0.0558743\n",
      "  0.06206934  0.05746542  0.05728124  0.05923583  0.0571107   0.05088812\n",
      "  0.0526209   0.06064471  0.04755408  0.04908689  0.04802066  0.05448622\n",
      "  0.05840299  0.05357353  0.05855299  0.05486383  0.05473874  0.05153489]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Johan jacket 2016-07-28 09:40:26.981459\n",
      "[ 0.23566818  0.24928377  0.22852363  0.25531084  0.24698933  0.21977748\n",
      "  0.22973597  0.22569622  0.22847913  0.23013342  0.22417708  0.24130113\n",
      "  0.22225437  0.23493191  0.23406077  0.22976332  0.23590937  0.22913636\n",
      "  0.22399473  0.22975634  0.23485202  0.24030422  0.23102925  0.22833704]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "box !!! 2016-07-28 09:40:59.739804\n",
      "[ 0.14137795  0.12408943  0.13563365  0.12908132  0.14244417  0.13694851\n",
      "  0.13589883  0.14482821  0.14014542  0.14146222  0.1351339   0.13002183\n",
      "  0.13498672  0.1220659   0.13623293  0.12599822  0.14178345  0.12879494\n",
      "  0.13581532  0.13913181  0.12519709  0.13569864  0.13212388  0.13791639]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "2nd backpack 2016-07-28 09:41:47.261237\n",
      "[ 0.2151594   0.22295723  0.22409082  0.23100515  0.21505414  0.21291091\n",
      "  0.20048073  0.22100498  0.23577818  0.19096598  0.21456973  0.23258814\n",
      "  0.2207952   0.21844225  0.23468725  0.21019554  0.20529655  0.20117888\n",
      "  0.20402417  0.20963542  0.20889615  0.22556135  0.22967985  0.20563717]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "couch 2016-07-28 09:42:30.687324\n",
      "[ 0.127438    0.13598535  0.13585881  0.12764785  0.13941926  0.13072298\n",
      "  0.13625725  0.12943688  0.12477793  0.1213914   0.12331836  0.12803323\n",
      "  0.12888058  0.1201676   0.12679448  0.12412946  0.12166357  0.11589574\n",
      "  0.1201976   0.13465353  0.14435804  0.13937237  0.12766568  0.1317337 ]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "TV 2016-07-28 09:43:02.561101\n",
      "[ 0.04573495  0.0407225   0.04716832  0.03576409  0.04158967  0.03668496\n",
      "  0.03819076  0.04065216  0.04066025  0.03298821  0.03379812  0.03763994\n",
      "  0.03432575  0.03734897  0.03771677  0.03073911  0.03977872  0.03659938\n",
      "  0.04656843  0.04318888  0.04210791  0.04037901  0.03659192  0.04003208]\n"
     ]
    }
   ],
   "source": [
    "import pymc3 as pm\n",
    "from theano import shared\n",
    "import datetime\n",
    "\n",
    "learned_probabilities = pd.DataFrame()\n",
    "now = datetime.datetime.now()\n",
    "print ('start : ', str(now))\n",
    "#for object_name in kth_dataset.drop('sep_hour', axis=1):\n",
    "for object_name in list_to_learn[25:]: \n",
    "    with pm.Model() as model: # model specifications in PyMC3 are wrapped in a with-statement\n",
    "        # Define random variables\n",
    "        grp_mean = pm.Beta('grp mean', alpha=2, beta=2) # prior\n",
    "        grp_scale = pm.Gamma('grp scale', alpha=1, beta=10./10**2) # prior\n",
    "\n",
    "        # Transform\n",
    "        alpha = grp_mean * grp_scale\n",
    "        beta = (1 - grp_mean) * grp_scale\n",
    "\n",
    "        # Individual random variables, vector of lenght 20\n",
    "        theta = pm.Beta('theta', alpha=alpha, beta=beta, shape=24)\n",
    "\n",
    "        # Define how data relates to unknown causes\n",
    "        data = pm.Bernoulli('observed',\n",
    "                            p=theta[kth_dataset['sep_hour']],\n",
    "                            observed=kth_dataset[object_name])\n",
    "\n",
    "        # Inference!\n",
    "        start = pm.find_MAP() # Find good starting point\n",
    "        step = pm.NUTS (scaling=start) # Instantiate MCMC sampling algorithm\n",
    "        trace = pm.sample(100, step, start=start, progressbar=False)[:20] # draw posterior samples using slice sampling\n",
    "    \n",
    "    print (object_name, str(datetime.datetime.now()))\n",
    "    print (np.mean(trace['theta'], axis=0))\n",
    "    learned_probabilities[object_name] = np.mean(trace['theta'], axis=0)\n",
    "    learned_probabilities.to_csv('./learned_probabilities_from25.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start :  2016-07-27 23:29:10.618094\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Nils chair 2016-07-27 23:30:03.438074\n",
      "[ 0.93966496  0.93798359  0.93479042  0.94156171  0.93859001  0.93860486\n",
      "  0.9392956   0.94393973  0.94730999  0.94752204  0.94817441  0.9504356\n",
      "  0.94869154  0.95219734  0.94923294  0.92805755  0.95108892  0.94808929\n",
      "  0.94498545  0.94069789  0.94152508  0.93505968  0.93866857  0.94116476]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Johan chair 2016-07-27 23:30:37.468990\n",
      "[ 0.95310728  0.95473553  0.95204452  0.95270151  0.95479633  0.95477537\n",
      "  0.95651758  0.95517668  0.95677637  0.95818991  0.96131502  0.95755229\n",
      "  0.95580612  0.95934193  0.96246809  0.96005341  0.96263909  0.95925014\n",
      "  0.95489399  0.95628862  0.95371772  0.95296416  0.95536603  0.95457517]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Yuquan chair 2016-07-27 23:31:12.167506\n",
      "[ 0.93471533  0.93425541  0.93229898  0.93376181  0.93502033  0.93485594\n",
      "  0.93281832  0.93461744  0.94200564  0.94516523  0.94580309  0.94057792\n",
      "  0.94529863  0.94788299  0.92858238  0.92884643  0.94962081  0.94652975\n",
      "  0.9415846   0.93442916  0.93449198  0.93661273  0.93203456  0.93485729]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Yuquan laptop 2016-07-27 23:31:50.522100\n",
      "[ 0.59570024  0.59995412  0.59665373  0.59729026  0.60178662  0.601336\n",
      "  0.59693063  0.61019404  0.60622256  0.63709973  0.63599791  0.64693264\n",
      "  0.61115168  0.60126369  0.55884164  0.57255934  0.56912577  0.59091634\n",
      "  0.60888508  0.59611177  0.60168686  0.60117712  0.59464286  0.60074132]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Nils Backpack 2016-07-27 23:32:26.839102\n",
      "[ 0.56316853  0.56251228  0.55856892  0.56458586  0.55661103  0.56259015\n",
      "  0.56523738  0.5369854   0.54840061  0.60441152  0.57749321  0.56767966\n",
      "  0.59947461  0.57598625  0.59851429  0.56614149  0.55592988  0.50133671\n",
      "  0.58487922  0.56143327  0.57128772  0.56849651  0.55918496  0.57028322]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "trash can 2016-07-27 23:33:03.947517\n",
      "[ 0.55317206  0.55989858  0.5491321   0.55605298  0.55874451  0.55290929\n",
      "  0.55407356  0.5613829   0.55331545  0.56278761  0.57470654  0.55635092\n",
      "  0.53923672  0.55002144  0.54156218  0.54375904  0.54712788  0.56200257\n",
      "  0.55272058  0.550129    0.5547213   0.55391379  0.55253511  0.55587706]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Yuquan bag 2016-07-27 23:33:40.654232\n",
      "[ 0.33708947  0.33100491  0.33030452  0.33259524  0.3300732   0.32984937\n",
      "  0.32909494  0.33813182  0.34270275  0.35319153  0.34831957  0.33231243\n",
      "  0.30261462  0.32067728  0.29574577  0.34418141  0.31631519  0.3003381\n",
      "  0.30860488  0.32048365  0.32421676  0.33155806  0.33572713  0.32820498]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Yuquan lamp 2016-07-27 23:34:14.879127\n",
      "[ 0.07589671  0.07444169  0.07590645  0.07420654  0.07624176  0.07706116\n",
      "  0.07431291  0.07229059  0.06788686  0.08708767  0.06512862  0.06477739\n",
      "  0.06320633  0.08225537  0.08407787  0.05874591  0.06075231  0.06198269\n",
      "  0.06683315  0.07151142  0.07633979  0.07851792  0.08162755  0.07812982]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Yuquan water bottle 2016-07-27 23:34:55.967975\n",
      "[ 0.05543049  0.0548866   0.05300215  0.05420416  0.05410408  0.05500359\n",
      "  0.05627703  0.05280805  0.04927157  0.05072385  0.049655    0.04919764\n",
      "  0.04970038  0.06392215  0.0495085   0.04905468  0.04545773  0.04721335\n",
      "  0.05361107  0.05135667  0.05698146  0.05634348  0.05521385  0.05492802]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "A bag on Johan's desk  2016-07-27 23:35:31.498003\n",
      "[ 0.08451442  0.07754035  0.0832892   0.08217465  0.08597154  0.07830543\n",
      "  0.08197516  0.07427643  0.07008619  0.06761539  0.0645152   0.06489165\n",
      "  0.06603467  0.06309033  0.08480431  0.06349646  0.08237254  0.06434264\n",
      "  0.07105     0.08156979  0.08189751  0.07977268  0.07560292  0.08578221]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "11 2016-07-27 23:36:20.090477\n",
      "[ 0.05117014  0.05143055  0.04623152  0.05531087  0.05112034  0.04970096\n",
      "  0.05044696  0.04150856  0.03590672  0.03254894  0.0339956   0.03479075\n",
      "  0.03017956  0.02986202  0.02963563  0.03069523  0.02958326  0.0306764\n",
      "  0.03964012  0.04467221  0.05183009  0.05426905  0.05917132  0.04630766]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Johan monitor 2016-07-27 23:36:55.856922\n",
      "[ 0.44641825  0.44790973  0.44580782  0.44499528  0.44625499  0.44402222\n",
      "  0.45040818  0.45094959  0.46199795  0.45358061  0.46298473  0.46486901\n",
      "  0.43943682  0.44190362  0.43431252  0.4445587   0.40950919  0.45297159\n",
      "  0.45278601  0.43586365  0.44330957  0.44605557  0.44830798  0.44628245]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Johan cup 2016-07-27 23:37:33.289223\n",
      "[ 0.31633324  0.30861949  0.31554956  0.31019057  0.32365779  0.31147933\n",
      "  0.3255697   0.3015664   0.31618101  0.3435925   0.27685601  0.26142172\n",
      "  0.32004825  0.31163597  0.28135917  0.36716325  0.30268869  0.34812532\n",
      "  0.30477037  0.29962118  0.31130126  0.31316346  0.30528342  0.31801422]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Yuquan cup 2016-07-27 23:38:08.484692\n",
      "[ 0.11081781  0.11613856  0.11626589  0.11710436  0.11994801  0.1153547\n",
      "  0.11531613  0.13306975  0.11610852  0.12162226  0.11424605  0.13226334\n",
      "  0.10281777  0.09847912  0.09663572  0.12521437  0.10979342  0.10007676\n",
      "  0.10648359  0.11364426  0.11367661  0.11466065  0.11712572  0.11560445]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Nils laptop 2016-07-27 23:38:43.861721\n",
      "[ 0.06240596  0.06652787  0.06143325  0.06285491  0.0626463   0.06221389\n",
      "  0.06452848  0.06293239  0.05906886  0.06037417  0.05808058  0.068828\n",
      "  0.06828499  0.05678018  0.05920132  0.05634461  0.05805552  0.05848903\n",
      "  0.06261762  0.06303175  0.06413919  0.06105262  0.06410267  0.06415544]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "16 2016-07-27 23:39:20.323173\n",
      "[ 0.06258398  0.06534739  0.0606517   0.06112984  0.06355973  0.06629133\n",
      "  0.06365563  0.05732973  0.05223717  0.0474712   0.05097709  0.04897539\n",
      "  0.04719942  0.04560952  0.04288574  0.04118083  0.04639065  0.0462427\n",
      "  0.05292723  0.05678821  0.06523651  0.05913961  0.06139529  0.06171716]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Pillow 2016-07-27 23:39:58.572620\n",
      "[ 0.47116899  0.47201039  0.47030399  0.47246057  0.46844655  0.46579965\n",
      "  0.47518936  0.47254657  0.47116276  0.4721919   0.48675415  0.45730371\n",
      "  0.47619847  0.48529226  0.45607601  0.46700585  0.47374915  0.477346\n",
      "  0.45642677  0.45986857  0.46973398  0.47127656  0.47215801  0.47063361]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Rares 2016-07-27 23:40:36.167452\n",
      "[ 0.08133485  0.07610598  0.07837626  0.08203218  0.07882268  0.081666\n",
      "  0.07859071  0.07891911  0.06716446  0.06865006  0.06473679  0.08740978\n",
      "  0.08936855  0.06192871  0.06207684  0.08352897  0.07984397  0.09017631\n",
      "  0.07385341  0.07823856  0.08424731  0.08152924  0.07940317  0.08032163]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "occulus 2016-07-27 23:41:11.750165\n",
      "[ 0.05097087  0.04933973  0.05166276  0.04617873  0.04981866  0.05098362\n",
      "  0.051966    0.04964781  0.04036047  0.03894126  0.03844523  0.03922687\n",
      "  0.04023908  0.03518085  0.03424866  0.03496175  0.03737524  0.06648184\n",
      "  0.03991385  0.04698855  0.05047316  0.04765005  0.05272383  0.04934495]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "lunch box  2016-07-27 23:41:49.074318\n",
      "[ 0.1561685   0.15895995  0.16297952  0.15806731  0.15771337  0.15806533\n",
      "  0.16435183  0.18050806  0.14116063  0.17848217  0.13580092  0.16769824\n",
      "  0.13354266  0.15105129  0.16291998  0.14575224  0.14214071  0.17142196\n",
      "  0.1657778   0.15299173  0.16497925  0.15471415  0.15355027  0.16043099]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Nils jacket 2016-07-27 23:42:22.696277\n",
      "[ 0.35418248  0.35563949  0.35209124  0.35695942  0.36083725  0.35502187\n",
      "  0.35627648  0.36234294  0.34956325  0.39850427  0.35622775  0.3754594\n",
      "  0.37079801  0.368105    0.36044641  0.36250109  0.35303398  0.33285061\n",
      "  0.33032916  0.33300202  0.3548506   0.35915588  0.35493302  0.35818781]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "22 2016-07-27 23:43:12.254231\n",
      "[ 0.07492432  0.07728457  0.07306268  0.07425019  0.07387329  0.07460026\n",
      "  0.07552802  0.07212516  0.06881634  0.06917073  0.06802102  0.06678762\n",
      "  0.06564959  0.06557053  0.06520021  0.06223176  0.06444611  0.06428076\n",
      "  0.06810747  0.07224843  0.07383806  0.07444476  0.07499296  0.07345324]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "23 2016-07-27 23:43:44.580684\n",
      "[ 0.14895517  0.15316128  0.15174514  0.1533903   0.15448779  0.15157281\n",
      "  0.15847717  0.14845112  0.1410029   0.15207099  0.16440417  0.13527162\n",
      "  0.1633309   0.1581032   0.15587239  0.12939926  0.14747186  0.1335422\n",
      "  0.14883681  0.14799778  0.15151506  0.15591875  0.15417558  0.15018758]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "24 2016-07-27 23:44:22.335117\n",
      "[ 0.090893    0.08988392  0.08780949  0.08850411  0.09137098  0.0897332\n",
      "  0.09555443  0.08629341  0.08323192  0.07977667  0.07295445  0.09717761\n",
      "  0.09842261  0.09305343  0.09194724  0.08699969  0.08805136  0.0764497\n",
      "  0.08094518  0.09213634  0.08874548  0.08941862  0.09171477  0.09253484]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Johan backpack 2016-07-27 23:44:54.936574\n",
      "[ 0.08471493  0.08394222  0.08498926  0.08462698  0.08510221  0.08521476\n",
      "  0.08458311  0.08494525  0.07918804  0.0779651   0.08785115  0.07670463\n",
      "  0.08652868  0.08899483  0.0845552   0.0859298   0.08349945  0.077788\n",
      "  0.07988786  0.08179816  0.08470009  0.08590687  0.0852687   0.08372956]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Table cloth 2016-07-27 23:45:33.667816\n",
      "[ 0.04325974  0.04114472  0.04186405  0.03600262  0.04112478  0.0390046\n",
      "  0.03638144  0.04078079  0.03240557  0.03075378  0.02941865  0.03146772\n",
      "  0.02714697  0.03004776  0.05807838  0.055138    0.03345044  0.03104899\n",
      "  0.04003146  0.04000503  0.03826125  0.0440721   0.04231547  0.0416514 ]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "27 2016-07-27 23:46:19.789596\n",
      "[ 0.05691122  0.05952115  0.05933598  0.0553928   0.0548763   0.05384852\n",
      "  0.05414388  0.05758018  0.04594662  0.04492304  0.04172206  0.04066486\n",
      "  0.0412549   0.04114815  0.042428    0.03561759  0.03452733  0.0414875\n",
      "  0.04746619  0.05168944  0.05791742  0.05432073  0.05376333  0.05602726]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Johan cap 2016-07-27 23:46:59.787464\n",
      "[ 0.08932123  0.08596944  0.09141903  0.08708142  0.07513438  0.08947045\n",
      "  0.09350512  0.07941414  0.06178449  0.10932757  0.09557336  0.05791344\n",
      "  0.09978107  0.0554092   0.09312796  0.05581625  0.0559516   0.05824468\n",
      "  0.06557596  0.07748474  0.09388417  0.09218877  0.08739354  0.09403586]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "calibration board 2016-07-27 23:47:55.581283\n",
      "[ 0.32504926  0.32263283  0.31993335  0.31979763  0.32431076  0.32507707\n",
      "  0.32226627  0.31144485  0.29977899  0.35118851  0.36301637  0.34767856\n",
      "  0.32333988  0.33239161  0.31406345  0.3076267   0.29252643  0.30404414\n",
      "  0.2957791   0.30721795  0.32205274  0.32121258  0.32382926  0.31998451]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Nils bike helmet 2016-07-27 23:48:29.038086\n",
      "[ 0.10050322  0.10089048  0.0999173   0.10016043  0.10145247  0.10020927\n",
      "  0.10190609  0.1004615   0.09515684  0.09594307  0.10497802  0.10276783\n",
      "  0.11264267  0.1015119   0.08947843  0.08870198  0.09107314  0.09195597\n",
      "  0.09677784  0.09961744  0.10077902  0.10385961  0.10132988  0.09868737]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Johan laptop 2016-07-27 23:49:09.416823\n",
      "[ 0.25581329  0.25589007  0.25008321  0.24774309  0.25399643  0.26014328\n",
      "  0.24608212  0.26410755  0.26400947  0.25914622  0.2806095   0.27904876\n",
      "  0.22775674  0.2621092   0.23362556  0.22864645  0.22680822  0.24561104\n",
      "  0.26155958  0.27065852  0.25413773  0.25445297  0.25703038  0.2505245 ]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Nils cup 2016-07-27 23:50:53.678331\n",
      "[ 0.0497554   0.04758445  0.04553541  0.05046964  0.04686254  0.04833348\n",
      "  0.04917958  0.05092684  0.04045322  0.06704524  0.07089563  0.03827156\n",
      "  0.03804491  0.03980036  0.03566085  0.0325947   0.03303279  0.03682394\n",
      "  0.04312008  0.04581109  0.05343845  0.04817313  0.05069648  0.04783825]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "Johan jacket 2016-07-27 23:51:51.348139\n",
      "[ 0.1880353   0.20023325  0.18992356  0.19549034  0.1976466   0.19021934\n",
      "  0.1909566   0.20910674  0.21202207  0.18025059  0.16133359  0.2200681\n",
      "  0.19767574  0.17374036  0.20871511  0.16497162  0.18820226  0.17690573\n",
      "  0.17392521  0.18388227  0.19268337  0.19672222  0.19522441  0.19535613]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "box !!! 2016-07-27 23:52:48.884800\n",
      "[ 0.12912921  0.13402334  0.12698674  0.13049841  0.12775973  0.12874189\n",
      "  0.12931869  0.12626229  0.13260955  0.12917271  0.13165204  0.12535219\n",
      "  0.10897968  0.10775275  0.12177524  0.10187961  0.11940663  0.12465092\n",
      "  0.11986909  0.12607211  0.13212918  0.12341102  0.12933793  0.12964113]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "2nd backpack 2016-07-28 00:53:54.128322\n",
      "[ 0.19890251  0.20073607  0.19565978  0.19524473  0.19827812  0.20165008\n",
      "  0.20122783  0.18719383  0.21730421  0.18325032  0.18723739  0.22756167\n",
      "  0.20221337  0.17752186  0.213483    0.17749787  0.18662593  0.18979019\n",
      "  0.1788486   0.18657127  0.19576754  0.19894472  0.19493065  0.19452221]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "couch 2016-07-28 00:55:20.983565\n",
      "[ 0.07376203  0.0750074   0.07171717  0.0744234   0.07211207  0.07264736\n",
      "  0.07132724  0.06578331  0.06168269  0.06340395  0.05968876  0.06358822\n",
      "  0.05650442  0.07573283  0.07309333  0.07625097  0.07407166  0.0606078\n",
      "  0.06473216  0.07028246  0.06711795  0.07410416  0.0724906   0.07022893]\n",
      "Applied logodds-transform to grp mean and added transformed grp mean_logodds to model.\n",
      "Applied log-transform to grp scale and added transformed grp scale_log to model.\n",
      "Applied logodds-transform to theta and added transformed theta_logodds to model.\n",
      "TV 2016-07-28 00:56:11.832130\n",
      "[ 0.06213153  0.05617707  0.06092559  0.06242147  0.06369215  0.06339307\n",
      "  0.06311359  0.05233159  0.04719729  0.04972324  0.04621897  0.05054035\n",
      "  0.04501451  0.04409937  0.04199486  0.04444726  0.03971711  0.04255689\n",
      "  0.0550921   0.05872848  0.07073601  0.06533155  0.06622794  0.05904344]\n"
     ]
    }
   ],
   "source": [
    "import pymc3 as pm\n",
    "learned_probabilities = pd.DataFrame()\n",
    "now = datetime.datetime.now()\n",
    "print ('start : ', str(now))\n",
    "#for object_name in kth_dataset.drop('sep_hour', axis=1):\n",
    "for object_name in list_to_learn[25:]: \n",
    "    with pm.Model() as model: # model specifications in PyMC3 are wrapped in a with-statement\n",
    "        # Define random variables\n",
    "        grp_mean = pm.Beta('grp mean', alpha=2, beta=2) # prior\n",
    "        grp_scale = pm.Gamma('grp scale', alpha=1, beta=10./10**2) # prior\n",
    "\n",
    "        # Transform\n",
    "        alpha = grp_mean * grp_scale\n",
    "        beta = (1 - grp_mean) * grp_scale\n",
    "\n",
    "        # Individual random variables, vector of lenght 20\n",
    "        theta = pm.Beta('theta', alpha=alpha, beta=beta, shape=24)\n",
    "\n",
    "        # Define how data relates to unknown causes\n",
    "        data = pm.Bernoulli('observed',\n",
    "                            p=theta[kth_dataset['sep_hour']],\n",
    "                            observed=kth_dataset[object_name])\n",
    "\n",
    "        # Inference!\n",
    "        start = pm.find_MAP() # Find good starting point\n",
    "        step = pm.NUTS (scaling=start) # Instantiate MCMC sampling algorithm\n",
    "        trace = pm.sample(1000, step, start=start, progressbar=False)[:200] # draw posterior samples using slice sampling\n",
    "    \n",
    "    print (object_name, str(datetime.datetime.now()))\n",
    "    print (np.mean(trace['theta'], axis=0))\n",
    "    learned_probabilities[object_name] = np.mean(trace['theta'], axis=0)\n",
    "    learned_probabilities.to_csv('./learned_probabilities_1000_from25.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_probabilities['trash can']\n",
    "kth_dataset['trash can'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'Nils chair',\n",
       " 'Johan chair',\n",
       " 'Yuquan chair',\n",
       " 'Yuquan laptop',\n",
       " 'Nils Backpack',\n",
       " 'trash can',\n",
       " 'Yuquan bag',\n",
       " 'Yuquan lamp',\n",
       " 'Yuquan water bottle',\n",
       " 'bag',\n",
       " '11',\n",
       " 'Johan monitor',\n",
       " 'Johan cup',\n",
       " 'Yuquan cup',\n",
       " 'Nils laptop',\n",
       " '16',\n",
       " 'Pillow',\n",
       " 'Rares',\n",
       " 'occulus',\n",
       " 'lunch box ',\n",
       " 'Nils jacket',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " 'Johan backpack',\n",
       " 'Table cloth',\n",
       " '27',\n",
       " 'Johan cap',\n",
       " 'calibration board',\n",
       " 'Nils bike helmet',\n",
       " 'Johan laptop',\n",
       " 'Nils cup',\n",
       " 'Johan jacket',\n",
       " 'box !!!',\n",
       " '2nd backpack',\n",
       " 'couch',\n",
       " 'TV']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_probabilities = pd.read_csv('./learned_probabilities_1000.csv')\n",
    "names = learned_probabilities.columns.tolist()\n",
    "names[10] = 'bag' names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "n_observations = kth_test_dataset.shape[0]\n",
    "\n",
    "all_evaluation = []\n",
    "for i in np.arange(10):\n",
    "    evaluation = {}\n",
    "    for object_name in kth_test_dataset.drop('sep_hour', axis=1):\n",
    "        count = 0\n",
    "        for index in kth_test_dataset[object_name].index:\n",
    "            expected_output = kth_test_dataset[object_name][index]\n",
    "            test_output = sp.stats.bernoulli(learned_probabilities[object_name][index.hour]).rvs(1)\n",
    "            if int(expected_output) == int(test_output[0]):\n",
    "                #print (\" E : \", expected_output, \" T : \", test_output)\n",
    "                count = count + 1\n",
    "\n",
    "        evaluation[object_name] = np.round(count/n_observations, 4)\n",
    "        \n",
    "    all_evaluation.append(evaluation)\n",
    "\n",
    "df = pd.DataFrame(all_evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "boxplot() got an unexpected keyword argument 'x_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-2b4215da7577>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib qt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mylim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Object name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Accuracy score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/deebuls/anaconda3/lib/python3.5/site-packages/pandas/tools/plotting.py\u001b[0m in \u001b[0;36mbox\u001b[1;34m(self, by, **kwds)\u001b[0m\n\u001b[0;32m   3812\u001b[0m         \u001b[0maxes\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAxesSubplot\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthem\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3813\u001b[0m         \"\"\"\n\u001b[1;32m-> 3814\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'box'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3816\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/deebuls/anaconda3/lib/python3.5/site-packages/pandas/tools/plotting.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\u001b[0m\n\u001b[0;32m   3735\u001b[0m                           \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolormap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3736\u001b[0m                           \u001b[0myerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecondary_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3737\u001b[1;33m                           sort_columns=sort_columns, **kwds)\n\u001b[0m\u001b[0;32m   3738\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplot_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/deebuls/anaconda3/lib/python3.5/site-packages/pandas/tools/plotting.py\u001b[0m in \u001b[0;36mplot_frame\u001b[1;34m(data, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\u001b[0m\n\u001b[0;32m   2609\u001b[0m                  \u001b[0myerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2610\u001b[0m                  \u001b[0msecondary_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2611\u001b[1;33m                  **kwds)\n\u001b[0m\u001b[0;32m   2612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/deebuls/anaconda3/lib/python3.5/site-packages/pandas/tools/plotting.py\u001b[0m in \u001b[0;36m_plot\u001b[1;34m(data, x, y, subplots, ax, kind, **kwds)\u001b[0m\n\u001b[0;32m   2436\u001b[0m         \u001b[0mplot_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubplots\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2438\u001b[1;33m     \u001b[0mplot_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2439\u001b[0m     \u001b[0mplot_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2440\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mplot_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/deebuls/anaconda3/lib/python3.5/site-packages/pandas/tools/plotting.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_plot_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_subplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_legend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/deebuls/anaconda3/lib/python3.5/site-packages/pandas/tools/plotting.py\u001b[0m in \u001b[0;36m_make_plot\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2329\u001b[0m             ret, bp = self._plot(ax, y, column_num=0,\n\u001b[1;32m-> 2330\u001b[1;33m                                  return_type=self.return_type, **kwds)\n\u001b[0m\u001b[0;32m   2331\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_color_bp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2332\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_return_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/deebuls/anaconda3/lib/python3.5/site-packages/pandas/tools/plotting.py\u001b[0m in \u001b[0;36m_plot\u001b[1;34m(cls, ax, y, column_num, return_type, **kwds)\u001b[0m\n\u001b[0;32m   2245\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2246\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_na\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2247\u001b[1;33m         \u001b[0mbp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'dict'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/deebuls/anaconda3/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1810\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[0;32m   1811\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1812\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1813\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1814\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: boxplot() got an unexpected keyword argument 'x_label'"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "df.plot.box(ylim=(0,1), rot=90, \n",
    "            x_label = \"Object name\", \n",
    "            y_label = \"Accuracy score\",\n",
    "            title = \"Object location knowledge evalutation\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deebuls/anaconda3/lib/python3.5/site-packages/matplotlib/axes/_axes.py:519: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    }
   ],
   "source": [
    "sns.heatmap(learned_probabilities.T)\n",
    "plt.title('Posteriors')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
